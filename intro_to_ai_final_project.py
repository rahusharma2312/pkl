# -*- coding: utf-8 -*-
"""Intro to AI_Final Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ds2RRWVGkoONkcAGuCjjS0heHaM5mLYz
"""

import random
import json

# Predefined inputs and responses
inputs = [
    "I am so stressed about my exams, what should I do?",
    "I can’t manage my time, I keep procrastinating!",
    "I’ve left everything for the last minute!",
    "I feel overwhelmed with all the assignments.",
    "I’m too tired to study anymore.",
    "I’m feeling anxious about the upcoming presentation.",
    "How do I get rid of stress before an exam?",
    "I’m not feeling motivated to study.",
    "How do I deal with constant distractions?",
    "I feel like giving up. Everything feels hard."
]

responses = [
    "It’s okay to feel stressed before exams. Try to break your tasks into smaller chunks, take short breaks, and focus on one thing at a time.",
    "Procrastination can be tough. Start by setting small, achievable goals, and give yourself a reward when you complete them.",
    "It happens! Start by creating a to-do list. Prioritize tasks and try to focus on one task at a time. You’ve got this!",
    "Take a deep breath! Make a plan by breaking down assignments into smaller tasks. Focus on one task at a time to avoid feeling overwhelmed.",
    "It’s important to rest. Try a short power nap or go for a walk to recharge. A refreshed mind learns better!",
    "It’s normal to feel anxious. Prepare well, practice in front of a mirror, and breathe deeply before presenting to calm your nerves.",
    "Try deep breathing exercises, take short breaks while studying, and remember that a little stress can help you focus better.",
    "Start small! Set a timer for 10-15 minutes and commit to studying for that time. You might find that momentum builds after you start.",
    "Minimize distractions by turning off notifications and finding a quiet study space. Use techniques like the Pomodoro method to stay focused.",
    "It’s okay to feel like that sometimes. Take a break, talk to someone you trust, and remember that challenges are part of growth. You’re not alone."
]

# Generate 1000 unique data points
dataset = []
for _ in range(1000):
    input_text = random.choice(inputs)
    response_text = random.choice(responses)
    dataset.append({"input": input_text, "response": response_text})

# Save dataset as a JSON file
with open("mental_wellbeing_chatbot_dataset.json", "w") as outfile:
    json.dump(dataset, outfile, indent=4)

print("Dataset generation complete! Saved to 'mental_wellbeing_chatbot_dataset.json'")

from datasets import load_dataset

# Load the dataset from the file
dataset = load_dataset("json", data_files={"train": "mental_wellbeing_chatbot_dataset.json"})

# Function to preprocess data for GPT-2 model
def preprocess_data(examples):
    inputs = examples['input']
    responses = examples['response']

    # Tokenizing inputs and responses
    input_encodings = tokenizer(inputs, truncation=True, padding=True, max_length=50)
    target_encodings = tokenizer(responses, truncation=True, padding=True, max_length=50)

    # Adding labels for training
    input_encodings['labels'] = target_encodings['input_ids']

    return input_encodings

# Apply the preprocessing
train_dataset = dataset['train'].map(preprocess_data, batched=True)

# Save the preprocessed data (Optional)
train_dataset.save_to_disk("processed_chatbot_dataset")

from transformers import GPT2Tokenizer

# Load GPT-2 tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# Add a padding token (using eos_token as padding)
tokenizer.pad_token = tokenizer.eos_token

from datasets import load_dataset
from transformers import GPT2Tokenizer

# Load the dataset from the file
dataset = load_dataset("json", data_files={"train": "mental_wellbeing_chatbot_dataset.json"})

# Load GPT-2 tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# Add a padding token to the tokenizer
tokenizer.pad_token = tokenizer.eos_token # using eos_token as pad_token
# or
# tokenizer.add_special_tokens({'pad_token': '[PAD]'}) # adding a new pad token '[PAD]'

# Function to preprocess data for GPT-2 model
def preprocess_data(examples):
    inputs = examples['input']
    responses = examples['response']

    # Tokenizing inputs and responses
    input_encodings = tokenizer(inputs, truncation=True, padding=True, max_length=50)
    target_encodings = tokenizer(responses, truncation=True, padding=True, max_length=50)

    # Adding labels for training
    input_encodings['labels'] = target_encodings['input_ids']

    return input_encodings

# Apply the preprocessing
train_dataset = dataset['train'].map(preprocess_data, batched=True)

# Save the preprocessed data (Optional)
train_dataset.save_to_disk("processed_chatbot_dataset")

def preprocess_data(examples):
    inputs = examples['input']
    responses = examples['response']

    # Tokenize the inputs and responses
    input_encodings = tokenizer(inputs, truncation=True, padding='max_length', max_length=50, return_tensors="pt")
    target_encodings = tokenizer(responses, truncation=True, padding='max_length', max_length=50, return_tensors="pt")

    # Ensure the labels are aligned with the inputs (the model expects labels to have the same length as inputs)
    input_encodings['labels'] = target_encodings['input_ids']

    # Flatten to remove batch dimension and ensure the padding works
    return {k: v.squeeze() for k, v in input_encodings.items()}

def preprocess_data(examples):
    inputs = examples['input']
    responses = examples['response']

    # Tokenize inputs and responses while ensuring padding and truncation are consistent
    input_encodings = tokenizer(inputs, truncation=True, padding='max_length', max_length=50, return_tensors="pt")
    target_encodings = tokenizer(responses, truncation=True, padding='max_length', max_length=50, return_tensors="pt")

    # Adding labels for training (the response sequence is the target label for training)
    input_encodings['labels'] = target_encodings['input_ids']

    # Ensure both input and labels have the same length
    return input_encodings

def preprocess_data(examples):
    inputs = examples['input']
    responses = examples['response']

    # Tokenize the inputs and responses with padding and truncation to a fixed length
    input_encodings = tokenizer(inputs, truncation=True, padding='max_length', max_length=50, return_tensors="pt")
    target_encodings = tokenizer(responses, truncation=True, padding='max_length', max_length=50, return_tensors="pt")

    # Ensure the labels are aligned with the inputs (the model expects labels to have the same length as inputs)
    # Both the inputs and labels should have the same length after padding
    input_encodings['labels'] = target_encodings['input_ids']

    # Ensure both input and label sequences have no extra batch dimensions or misalignment
    return {k: v.squeeze() for k, v in input_encodings.items()}

def preprocess_data(examples):
    inputs = examples['input']
    responses = examples['response']

    # Tokenize the inputs and responses with padding and truncation to a fixed length
    input_encodings = tokenizer(
        inputs,
        truncation=True,
        padding='max_length',
        max_length=50,
        return_tensors="pt"
    )
    target_encodings = tokenizer(
        responses,
        truncation=True,
        padding='max_length',
        max_length=50,
        return_tensors="pt"
    )

    # Ensure the labels are aligned with the inputs (the model expects labels to have the same length as inputs)
    # Both the inputs and labels should have the same length after padding
    input_encodings['labels'] = target_encodings['input_ids']

    # **Change:** Remove the squeeze operation to maintain batch dimensions
    # return {k: v.squeeze() for k, v in input_encodings.items()}
    return input_encodings # Return the dictionary as is

def preprocess_data(examples):
    inputs = examples['input']
    responses = examples['response']

    # Tokenize the inputs and responses with padding and truncation
    input_encodings = tokenizer(inputs, truncation=True, padding='max_length', max_length=50, return_tensors="pt")
    target_encodings = tokenizer(responses, truncation=True, padding='max_length', max_length=50, return_tensors="pt")

    # Ensure the labels are aligned with the inputs (the model expects labels to have the same length as inputs)
    input_encodings['labels'] = target_encodings['input_ids']

    # Ensure that input and labels are correctly aligned and don't have extra batch dimensions
    return {k: v.squeeze() for k, v in input_encodings.items()}

def preprocess_data(examples):
    inputs = examples['input']
    responses = examples['response']

    # Tokenize the inputs and responses with padding and truncation
    input_encodings = tokenizer(inputs, truncation=True, padding='max_length', max_length=50, return_tensors="pt")
    target_encodings = tokenizer(responses, truncation=True, padding='max_length', max_length=50, return_tensors="pt")

    # Ensure the labels are aligned with the inputs (the model expects labels to have the same length as inputs)
    input_encodings['labels'] = target_encodings['input_ids']

    # Debug: Print the shapes of input_ids and labels
    print(f"Input shape: {input_encodings['input_ids'].shape}")
    print(f"Labels shape: {input_encodings['labels'].shape}")

    # Ensure that input and labels are correctly aligned and don't have extra batch dimensions
    return {k: v.squeeze() for k, v in input_encodings.items()}

# Adjust the TrainingArguments to use eval_strategy instead of evaluation_strategy
training_args = TrainingArguments(
    output_dir="./chatbot_model",  # Directory to save the model
    eval_strategy="no",  # Corrected eval_strategy
    learning_rate=5e-5,  # Learning rate for training
    per_device_train_batch_size=4,  # Batch size for training
    num_train_epochs=3,  # Number of training epochs
    save_steps=1000,  # Save the model every 1000 steps
    logging_dir='./logs',  # Directory for logging
)

def preprocess_data(examples):
    inputs = examples['input']
    responses = examples['response']

    # Tokenize inputs with padding and truncation, both for inputs and responses
    input_encodings = tokenizer(inputs, truncation=True, padding='max_length', max_length=50, return_tensors="pt")
    target_encodings = tokenizer(responses, truncation=True, padding='max_length', max_length=50, return_tensors="pt")

    # Align input_encodings with target_encodings to make sure labels are in the right shape
    input_encodings['labels'] = target_encodings['input_ids']

    # Return the encoded input and target
    return input_encodings

from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

# Load and preprocess the dataset
train_dataset = dataset["train"].map(preprocess_data, batched=True)

# Review the first example
print(train_dataset[0])

training_args = TrainingArguments(
    output_dir="./chatbot_model",  # Directory to save the model
    evaluation_strategy="no",  # Corrected eval_strategy
    learning_rate=5e-5,  # Learning rate for training
    per_device_train_batch_size=4,  # Batch size for training
    num_train_epochs=3,  # Number of training epochs
    save_steps=1000,  # Save the model every 1000 steps
    logging_dir='./logs',  # Directory for logging
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=data_collator,
)

# Start fine-tuning
trainer.train()

# Save the fine-tuned model
model.save_pretrained("./chatbot_model")
tokenizer.save_pretrained("./chatbot_model")

# Ensure this is the correct path to your model directory
model_path = "/content/drive/MyDrive/your_model_folder/chatbot_intent_model"  # Update with the actual path
tokenizer_path = "/content/drive/MyDrive/your_model_folder/chatbot_intent_model"

import os

# Check if the directory exists and contains the model files
model_path = "./chatbot_intent_model"
if os.path.exists(model_path) and os.path.isdir(model_path):
    print(f"Model directory found at {model_path}")
else:
    print(f"Model directory not found at {model_path}. Please check the path.")

import os

# Check the current working directory
print("Current Working Directory:", os.getcwd())

# Check if the model folder is in Google Drive (if you're using Google Colab)
from google.colab import drive
drive.mount('/content/drive')

# Check the model folder in your Google Drive
model_path = '/content/drive/MyDrive/chatbot_intent_model'
if os.path.exists(model_path) and os.path.isdir(model_path):
    print(f"Model directory found at {model_path}")
else:
    print(f"Model directory not found at {model_path}. Please check the path.")

# Re-save the model and tokenizer
model.save_pretrained('./chatbot_intent_model')
tokenizer.save_pretrained('./chatbot_intent_model')

# Re-save the model and tokenizer with the correct files
model.save_pretrained('./chatbot_intent_model')
tokenizer.save_pretrained('./chatbot_intent_model')

# Check if the files are now saved correctly
import os
print(os.listdir('./chatbot_intent_model'))

from transformers import AutoModelForSequenceClassification, AutoTokenizer

# Load the model and tokenizer
model = AutoModelForSequenceClassification.from_pretrained('./chatbot_intent_model')
tokenizer = AutoTokenizer.from_pretrained('./chatbot_intent_model')

# Check if the model and tokenizer are loaded successfully
print(model)
print(tokenizer)

from transformers import AutoModelForSequenceClassification, AutoTokenizer

# Load the model and tokenizer
model = AutoModelForSequenceClassification.from_pretrained('./chatbot_intent_model')
tokenizer = AutoTokenizer.from_pretrained('./chatbot_intent_model')

# Check if the model and tokenizer are loaded successfully
print(model)
print(tokenizer)

import torch

# Reverse mapping for intent labels (update this with your intent labels)
id_to_intent = {0: "anxiety_management", 1: "motivation"}  # Example intent labels

def get_intent(user_input):
    # Tokenize the input text (just like training)
    tokens = tokenizer(user_input, return_tensors="pt", truncation=True, padding="max_length", max_length=128)

    # Make the prediction
    with torch.no_grad():
        outputs = model(**tokens)

    # Get the predicted label (intent)
    predicted_label = torch.argmax(outputs.logits, dim=1).item()
    return id_to_intent.get(predicted_label, "Unknown")  # Map numeric label to the actual intent

# Test the function with some sample inputs
user_input = "I'm feeling anxious about my upcoming exam."
predicted_intent = get_intent(user_input)
print(f"Predicted Intent: {predicted_intent}")

import json
import random

# Original dataset (40 examples as shown above)
data = [
    {"intent": "greeting", "user": "Hello!", "response": "Hi there! How can I help you today?"},
    {"intent": "goodbye", "user": "Goodbye!", "response": "Goodbye! Have a wonderful day ahead!"},
    # Add the rest of your intents here...
]

# Function to generate more examples by modifying user input slightly
def expand_dataset(data, num_examples=1000):
    expanded_data = []
    for entry in data:
        for _ in range(num_examples // len(data)):
            modified_example = entry.copy()
            modified_example['user'] = modify_example(entry['user'])
            expanded_data.append(modified_example)
    return expanded_data

# Function to modify user input slightly (e.g., add more variations)
def modify_example(example):
    variations = [
        example,
        example.replace('!', '?'),
        example.lower(),
        example.upper()
    ]
    return random.choice(variations)

# Generate 1000 examples
expanded_data = expand_dataset(data, num_examples=1000)

# Save the expanded dataset
with open('expanded_chatbot_data.json', 'w') as f:
    json.dump(expanded_data, f, indent=2)

print("Expanded dataset saved successfully.")

from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch

# Load the trained model and tokenizer
model = AutoModelForSequenceClassification.from_pretrained('./chatbot_model')
tokenizer = AutoTokenizer.from_pretrained('./chatbot_model')

# Define a mapping from intent labels to human-readable intent names
intent_labels = {
    0: "greeting",
    1: "goodbye",
    2: "anxiety_management",
    3: "motivation",
    4: "stress_relief",
    5: "time_management",
    # Add more intents as per your training dataset
}

def get_intent(user_input):
    # Tokenize the user input
    inputs = tokenizer(user_input, return_tensors="pt", truncation=True, padding="max_length", max_length=128)

    # Get model output
    with torch.no_grad():
        outputs = model(**inputs)

    # Predict the intent (get the index of the maximum logit)
    predicted_label = torch.argmax(outputs.logits, dim=1).item()

    # Map predicted label to intent name
    predicted_intent = intent_labels.get(predicted_label, "Unknown")
    return predicted_intent

def get_intent(user_input):
    # Tokenize the user input
    inputs = tokenizer(user_input, return_tensors="pt", truncation=True, padding="max_length", max_length=128)

    # Get model output
    with torch.no_grad():
        outputs = model(**inputs)

    # Get the predicted label (index of maximum logit)
    predicted_label = torch.argmax(outputs.logits, dim=1).item()

    # Map the predicted label to intent
    predicted_intent = intent_labels.get(predicted_label, "Unknown")
    return predicted_intent

def get_intent(user_input):
    # Tokenize the user input
    inputs = tokenizer(user_input, return_tensors="pt", truncation=True, padding="max_length", max_length=128)

    # Get model output
    with torch.no_grad():
        outputs = model(**inputs)

    # Print logits to check the raw model predictions
    print("Logits:", outputs.logits)

    # Get the predicted label (index of maximum logit)
    predicted_label = torch.argmax(outputs.logits, dim=1).item()
    print("Predicted label:", predicted_label)

    # Map the predicted label to intent
    predicted_intent = intent_labels.get(predicted_label, "Unknown")
    return predicted_intent

# Assuming "bot" is the column for the responses
train_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "bot"])
val_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "bot"])

# Assuming "intent" is the column for the labels
train_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "intent"])
val_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "intent"])

print("Train Dataset Columns:", train_dataset.column_names)
print("Validation Dataset Columns:", val_dataset.column_names)

print("Sample from Train Dataset:", train_dataset[0])

print("Sample from Train Dataset:", train_dataset[0])

import json
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import AutoTokenizer

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

import json
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import AutoTokenizer


# Load the Study Wellness chatbot dataset
# Assuming the file is in your 'My Drive' folder
file_path = '/content/drive/My Drive/study_wellness_chatbot_dataset_1000.json'
with open(file_path, 'r') as f:
    data = json.load(f)

# Split the data into training and validation sets (80% training, 20% validation)
train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)

# Convert to Hugging Face Dataset format
train_dataset = Dataset.from_list(train_data)
val_dataset = Dataset.from_list(val_data)

# Load the tokenizer (BERT for this example)
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

# Tokenization function
def tokenize_function(example):
    return tokenizer(example["user"], truncation=True, padding="max_length", max_length=128)

train_dataset = train_dataset.map(tokenize_function, batched=True)
val_dataset = val_dataset.map(tokenize_function, batched=True)

# Prepare the datasets for PyTorch (set the format)
# Change 'response' to 'bot' to match the column name in your dataset
train_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "bot"])
val_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "bot"])

import json
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import AutoTokenizer

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Load the Study Wellness chatbot dataset
file_path = '/content/drive/My Drive/study_wellness_chatbot_dataset_1000.json'
with open(file_path, 'r') as f:
    data = json.load(f)

# Split the data into training and validation sets (80% training, 20% validation)
train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)

# Convert to Hugging Face Dataset format
train_dataset = Dataset.from_list(train_data)
val_dataset = Dataset.from_list(val_data)

# Load the tokenizer (BERT for this example)
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

# Tokenization function
def tokenize_function(example):
    return tokenizer(example["user"], truncation=True, padding="max_length", max_length=128)

# Apply tokenization
train_dataset = train_dataset.map(tokenize_function, batched=True)
val_dataset = val_dataset.map(tokenize_function, batched=True)

# Check available columns
print("Train Dataset Columns:", train_dataset.column_names)
print("Validation Dataset Columns:", val_dataset.column_names)

# Prepare the datasets for PyTorch (set the format)
# Assuming "bot" column contains the labels/responses
train_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "bot"])
val_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "bot"])

# Inspect the datasets to ensure proper format
print("Sample from Train Dataset:", train_dataset[0])
print("Sample from Validation Dataset:", val_dataset[0])

from huggingface_hub import notebook_login
notebook_login()


from transformers import AutoModelForCausalLM, AutoTokenizer

# Load GPT-2 model and tokenizer using AutoClasses
model = AutoModelForCausalLM.from_pretrained("gpt2")
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# Add a special token if needed (e.g., for separating user and bot messages)
special_token = "<|endoftext|>"
tokenizer.add_special_tokens({"pad_token": special_token})
model.resize_token_embeddings(len(tokenizer))

# Combine user input and bot responses into one text
formatted_data = []
for example in data:
    user_input = example["user"]
    bot_response = example["bot"]
    formatted_data.append(f"{user_input}{special_token}{bot_response}{special_token}")

from datasets import Dataset

# Create a Hugging Face Dataset
formatted_dataset = Dataset.from_dict({"text": formatted_data})

# Tokenize the data
def tokenize_function(example):
    return tokenizer(
        example["text"], truncation=True, padding="max_length", max_length=512
    )

tokenized_dataset = formatted_dataset.map(tokenize_function, batched=True)
tokenized_dataset.set_format(type="torch", columns=["input_ids", "attention_mask"])

# Split the dataset into training and validation sets
split_datasets = tokenized_dataset.train_test_split(test_size=0.2, seed=42)
train_data = split_datasets["train"]
val_data = split_datasets["test"]

print("Training Data Size:", len(train_data))
print("Validation Data Size:", len(val_data))

# Install necessary libraries (if not already installed)

# Import necessary libraries
import json
from transformers import GPT2LMHeadModel, GPT2Tokenizer, TrainingArguments, Trainer
from datasets import Dataset

# Load GPT-2 model and tokenizer
model = GPT2LMHeadModel.from_pretrained("gpt2")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# Add a special token if needed (e.g., for separating user and bot messages)
special_token = "<|endoftext|>"
tokenizer.add_special_tokens({"pad_token": special_token})
model.resize_token_embeddings(len(tokenizer))

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Load the Study Wellness chatbot dataset
file_path = '/content/drive/My Drive/study_wellness_chatbot_dataset_1000.json'
with open(file_path, 'r') as f:
    data = json.load(f)

# Prepare formatted data
# Assuming `data` is your original dataset with "user" and "bot" keys
formatted_data = []
for example in data:
    user_input = example["user"]
    bot_response = example["bot"]
    formatted_data.append(f"{user_input}{special_token}{bot_response}{special_token}")


# Create a Hugging Face Dataset
formatted_dataset = Dataset.from_dict({"text": formatted_data})

# Tokenize the data
def tokenize_function(example):
    # Tokenize the text and set labels to the input_ids shifted by one position
    tokenized = tokenizer(
        example["text"], truncation=True, padding="max_length", max_length=512
    )
    tokenized["labels"] = tokenized["input_ids"].copy()  # Set labels for loss calculation
    return tokenized

tokenized_dataset = formatted_dataset.map(tokenize_function, batched=True)
tokenized_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"]) # Include 'labels'

# Split the dataset into training and validation sets
split_datasets = tokenized_dataset.train_test_split(test_size=0.2, seed=42)
train_data = split_datasets["train"]
val_data = split_datasets["test"]

print("Training Data Size:", len(train_data))
print("Validation Data Size:", len(val_data))

# Set up training arguments
training_args = TrainingArguments(
    output_dir="./gpt2_results",
    evaluation_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=8,  # Adjust based on GPU memory
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    save_steps=500,
    save_total_limit=2,
    logging_dir="./logs",
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=val_data,
)

# Fine-tune GPT-2
trainer.train()

# Save the fine-tuned model
model.save_pretrained("./fine_tuned_gpt2")
tokenizer.save_pretrained("./fine_tuned_gpt2")

# Test the chatbot
from transformers import pipeline

chatbot = pipeline("text-generation", model="./fine_tuned_gpt2", tokenizer=tokenizer)

# Generate a response
user_input = "I feel anxious about my exams."
response = chatbot(f"{user_input}{special_token}", max_length=50, num_return_sequences=1)
print(response[0]["generated_text"])

# Install necessary libraries (if not already installed)
!pip install transformers datasets

# Import necessary libraries
import json
from transformers import GPT2LMHeadModel, GPT2Tokenizer, TrainingArguments, Trainer
from datasets import Dataset

# Load GPT-2 model and tokenizer (Consider using distilgpt2 for faster training)
model = GPT2LMHeadModel.from_pretrained("gpt2")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# Add a special token if needed (e.g., for separating user and bot messages)
special_token = "<|endoftext|>"
tokenizer.add_special_tokens({"pad_token": special_token})
model.resize_token_embeddings(len(tokenizer))

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Load the Study Wellness chatbot dataset
file_path = '/content/drive/My Drive/study_wellness_chatbot_dataset_1000.json'
with open(file_path, 'r') as f:
    data = json.load(f)

# Prepare formatted data
formatted_data = [
    f"{example['user']}{special_token}{example['bot']}{special_token}" for example in data
]

# Create a Hugging Face Dataset
formatted_dataset = Dataset.from_dict({"text": formatted_data})

# Tokenize the data
def tokenize_function(example):
    tokenized = tokenizer(
        example["text"], truncation=True, padding="max_length", max_length=256  # Reduced max_length
    )
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

tokenized_dataset = formatted_dataset.map(tokenize_function, batched=True)
tokenized_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

# Split the dataset into training and validation sets
split_datasets = tokenized_dataset.train_test_split(test_size=0.2, seed=42)
train_data = split_datasets["train"]
val_data = split_datasets["test"]

print("Training Data Size:", len(train_data))
print("Validation Data Size:", len(val_data))

# Set up optimized training arguments
training_args = TrainingArguments(
    output_dir="./gpt2_results",
    evaluation_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=8,  # Reduced batch size
    per_device_eval_batch_size=8,
    num_train_epochs=3,  # Lower epochs for initial testing
    weight_decay=0.01,
    save_steps=1000,  # Less frequent checkpoints
    save_total_limit=2,
    logging_dir="./logs",
    fp16=True,  # Enable mixed precision training
    logging_steps=200,  # Reduced logging frequency
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=val_data,
)

# Fine-tune GPT-2
trainer.train()

# Save the fine-tuned model
model.save_pretrained("./fine_tuned_gpt2")
tokenizer.save_pretrained("./fine_tuned_gpt2")

# Test the chatbot
from transformers import pipeline

chatbot = pipeline("text-generation", model="./fine_tuned_gpt2", tokenizer=tokenizer)

# Generate a response
user_input = "I feel anxious about my exams."
response = chatbot(f"{user_input}{special_token}", max_length=50, num_return_sequences=1)
print(response[0]["generated_text"])

# This code snippet is to create GUI interface for interaction with the
!pip install streamlit
import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load your fine-tuned GPT-2 model and tokenizer
model = AutoModelForCausalLM.from_pretrained("./fine_tuned_gpt2")
tokenizer = AutoTokenizer.from_pretrained("./fine_tuned_gpt2")
special_token = "<|endoftext|>"  # Your special token

# Streamlit app title
st.title("Mental Wellbeing Chatbot")

# User input area
user_input = st.text_input("You:")

# Generate response button
if st.button("Generate Response"):
    if user_input:
        # Generate response using the model
        input_ids = tokenizer.encode(user_input + special_token, return_tensors="pt")
        output = model.generate(input_ids, max_length=100, num_return_sequences=1)
        response = tokenizer.decode(output[0], skip_special_tokens=True)

        # Display the response
        st.write("Chatbot:", response)
    else:
        st.write("Please enter a message.")

!pip install streamlit
import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load your fine-tuned GPT-2 model and tokenizer
@st.cache_resource # Cache the model and tokenizer for faster loading
def load_model_and_tokenizer():
    """Loads and returns the pre-trained model and tokenizer."""
    model = AutoModelForCausalLM.from_pretrained("./fine_tuned_gpt2")
    tokenizer = AutoTokenizer.from_pretrained("./fine_tuned_gpt2")
    return model, tokenizer

model, tokenizer = load_model_and_tokenizer()
special_token = "<|endoftext|>"  # Your special token

# Streamlit app title
st.title("Mental Wellbeing Chatbot")

# User input area
user_input = st.text_input("You:")

# Generate response button
if st.button("Generate Response"):
    if user_input:
        # Generate response using the model
        input_ids = tokenizer.encode(user_input + special_token, return_tensors="pt")
        output = model.generate(input_ids, max_length=100, num_return_sequences=1)
        response = tokenizer.decode(output[0], skip_special_tokens=True)

        # Display the response
        st.write("Chatbot:", response)
    else:
        st.write("Please enter a message.")

!streamlit run app.py

!pip install streamlit pyngrok

import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load your fine-tuned model
@st.cache_resource
def load_model_and_tokenizer():
    model = AutoModelForCausalLM.from_pretrained("./fine_tuned_gpt2")
    tokenizer = AutoTokenizer.from_pretrained("./fine_tuned_gpt2")
    return model, tokenizer

model, tokenizer = load_model_and_tokenizer()

st.title("Mental Wellbeing Chatbot")
user_input = st.text_input("You:", placeholder="Type your message here...")

if st.button("Generate Response"):
    if user_input:
        input_ids = tokenizer.encode(user_input + "<|endoftext|>", return_tensors="pt")
        output = model.generate(input_ids, max_length=100, num_return_sequences=1)
        response = tokenizer.decode(output[0], skip_special_tokens=True)
        st.write(f"Chatbot: {response}")
    else:
        st.write("Please enter a message!")

ls

!ngrok config add-authtoken YOUR_AUTHTOKEN

from google.colab import drive
drive.mount('/content/drive')

with open('/content/drive/My Drive/app.py', 'w') as f:
    f.write("""
import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer

@st.cache_resource
def load_model_and_tokenizer():
    model = AutoModelForCausalLM.from_pretrained("./fine_tuned_gpt2")
    tokenizer = AutoTokenizer.from_pretrained("./fine_tuned_gpt2")
    return model, tokenizer

model, tokenizer = load_model_and_tokenizer()

st.title("Mental Wellbeing Chatbot")
user_input = st.text_input("You:", placeholder="Type your message here...")

if st.button("Generate Response"):
    if user_input:
        input_ids = tokenizer.encode(user_input + "<|endoftext|>", return_tensors="pt")
        output = model.generate(input_ids, max_length=100, num_return_sequences=1)
        response = tokenizer.decode(output[0], skip_special_tokens=True)
        st.write(f"Chatbot: {response}")
    else:
        st.write("Please enter a message!")
""")



# Commented out IPython magic to ensure Python compatibility.
# # Install necessary libraries
# !pip install streamlit pyngrok transformers
# 
# # Save the Streamlit app code as app.py
# %%writefile app.py
# import streamlit as st
# from transformers import AutoModelForCausalLM, AutoTokenizer
# 
# @st.cache_resource
# def load_model_and_tokenizer():
#     model = AutoModelForCausalLM.from_pretrained("./fine_tuned_gpt2")
#     tokenizer = AutoTokenizer.from_pretrained("./fine_tuned_gpt2")
#     return model, tokenizer
# 
# model, tokenizer = load_model_and_tokenizer()
# special_token = "<|endoftext|>"
# 
# st.title("Mental Wellbeing Chatbot")
# user_input = st.text_input("You:", placeholder="Type your message here...")
# 
# if st.button("Generate Response"):
#     if user_input:
#         input_ids = tokenizer.encode(user_input + special_token, return_tensors="pt")
#         output = model.generate(input_ids, max_length=100, num_return_sequences=1, top_p=0.9, temperature=0.7)
#         response = tokenizer.decode(output[0], skip_special_tokens=True)
#         st.write(f"Chatbot: {response}")
#     else:
#         st.write("Please enter a message!")
# 
# # Set up and run ngrok to expose the Streamlit app
# from pyngrok import ngrok
# !streamlit run app.py &  # Start Streamlit in the background
# public_url = ngrok.connect(8501)  # Expose Streamlit app
# print(f"Streamlit app is available at: {public_url}")
#

# Save the Streamlit app code as app.py

app_code = """
import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer

@st.cache_resource
def load_model_and_tokenizer():
    model = AutoModelForCausalLM.from_pretrained("./fine_tuned_gpt2")
    tokenizer = AutoTokenizer.from_pretrained("./fine_tuned_gpt2")
    return model, tokenizer

model, tokenizer = load_model_and_tokenizer()
special_token = "<|endoftext|>"

st.title("Mental Wellbeing Chatbot")
user_input = st.text_input("You:", placeholder="Type your message here...")

if st.button("Generate Response"):
    if user_input:
        input_ids = tokenizer.encode(user_input + special_token, return_tensors="pt")
        output = model.generate(input_ids, max_length=100, num_return_sequences=1, top_p=0.9, temperature=0.7)
        response = tokenizer.decode(output[0], skip_special_tokens=True)
        st.write(f"Chatbot: {response}")
    else:
        st.write("Please enter a message!")
"""

# Write the above code to the app.py file
with open('/content/app.py', 'w') as file:
    file.write(app_code)

print("Streamlit app saved as app.py")

from pyngrok import ngrok

# Start Streamlit in the background
!streamlit run /content/app.py &

# Expose the Streamlit app via ngrok
public_url = ngrok.connect(8501)
print(f"Streamlit App is available at: {public_url}")

!streamlit run app.py

!streamlit run app.py

import json
import random

# Define a base dataset structure
data = []

# Common topics and queries
queries = [
    "How do I manage my time better?",
    "I feel anxious about exams.",
    "How can I stay motivated to study?",
    "What’s the best way to prepare for science?",
    "Can you give me some study tips?",
    "How do I deal with procrastination?",
    "What’s the best way to revise for exams?",
    "How should I manage multiple subjects?",
    "I’m feeling overwhelmed. What should I do?",
    "How can I focus better while studying?",
    "What’s the Pomodoro technique?",
]

# Responses to common queries
responses = [
    "Create a schedule, prioritize tasks, and take regular breaks.",
    "It's normal to feel anxious. Try deep breathing and mindfulness exercises.",
    "Focus on the end goal and reward yourself for completing small tasks.",
    "Break the subject into topics and allocate specific time for each topic.",
    "Revise actively by summarizing topics and quizzing yourself.",
    "Break tasks into smaller, manageable chunks and start with one small step.",
    "Review your notes, create summaries, and test yourself with past papers.",
    "Allocate time slots for each subject and stick to your plan.",
    "Take a deep breath, list your priorities, and tackle them one at a time.",
    "Minimize distractions, take breaks, and study in a quiet place.",
    "Work for 25 minutes, then take a 5-minute break. Repeat for 4 cycles, then rest for 15 minutes.",
]

# Fallback responses for ambiguous or unclear queries
fallbacks = [
    "Can you clarify your question?",
    "Could you provide more details so I can help?",
    "I'm not sure I understand. Could you ask differently?",
    "Let's focus on one thing at a time. What do you need help with first?",
    "That's interesting! Can you tell me more about what you're asking?",
]

# Generate 1000 examples with some fallback cases
for i in range(1000):
    if i % 10 == 0:  # Every 10th example is a fallback
        # Select a random fallback response from the list
        fallback = random.choice(fallbacks)
        data.append({"user": "What else?", "bot": fallback})
    elif i % 15 == 0:  # Edge case for "and?" or similar
        data.append({"user": "and?", "bot": "Can you be more specific? What else would you like to know?"})
    else:  # Regular responses
        idx = i % len(queries)  # Cycle through queries and responses
        data.append({"user": queries[idx], "bot": responses[idx]})

# Save the dataset to a JSON file
file_path = "study_wellness_chatbot_dataset_1000.json"
with open(file_path, "w") as f:
    json.dump(data, f, indent=2)

print(f"Dataset saved to {file_path}")

!pip install transformers datasets

import datasets
import transformers

per_device_train_batch_size=4
per_device_eval_batch_size=4
gradient_accumulation_steps=4  # To simulate a larger effective batch size

import json
from transformers import (
    GPT2LMHeadModel,
    GPT2Tokenizer,
    Trainer,
    TrainingArguments,
)
from datasets import Dataset

# Load GPT-2 model and tokenizer (use 'distilgpt2' for faster training if needed)
model_name = "gpt2"  # Change to "distilgpt2" for smaller model
model = GPT2LMHeadModel.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

# Add a special token to separate user and bot messages
special_token = "<|endoftext|>"
tokenizer.add_special_tokens({"pad_token": special_token})
model.resize_token_embeddings(len(tokenizer))

# Step 1: Load Dataset
from google.colab import drive
drive.mount('/content/drive')

# Correct the file path
file_path = "/content/drive/My Drive/study_wellness_chatbot_dataset_1000.json"
with open(file_path, "r") as f:
    data = json.load(f)


# Prepare formatted data (User and Bot conversation)
formatted_data = [
    {
        "text": f"User: {example['user']}\nBot: {example['bot']}{special_token}"
    }
    for example in data
]

# Step 2: Convert to Hugging Face Dataset
dataset = Dataset.from_dict({"text": [item["text"] for item in formatted_data]})

# Step 3: Tokenize Dataset
def tokenize_function(example):
    tokens = tokenizer(
        example["text"],
        truncation=True,
        padding="max_length",
        max_length=256,
    )
    tokens["labels"] = tokens["input_ids"].copy()  # Set labels for loss calculation
    return tokens

tokenized_dataset = dataset.map(tokenize_function, batched=True)
tokenized_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

# Step 4: Split Dataset
train_test_split = tokenized_dataset.train_test_split(test_size=0.2, seed=42)
train_dataset = train_test_split["train"]
val_dataset = train_test_split["test"]

print(f"Training Data Size: {len(train_dataset)}")
print(f"Validation Data Size: {len(val_dataset)}")

# Step 5: Training Arguments
training_args = TrainingArguments(
    output_dir="./fine_tuned_gpt2",
    evaluation_strategy="epoch",  # Evaluate at the end of each epoch
    logging_dir="./logs",         # Log directory
    logging_steps=200,            # Log every 200 steps
    save_strategy="epoch",        # Save at the end of each epoch
    learning_rate=5e-5,           # Adjust learning rate for optimization
    per_device_train_batch_size=8,  # Adjust based on GPU memory
    per_device_eval_batch_size=8,
    num_train_epochs=3,           # Set lower for faster testing, increase for full training
    gradient_accumulation_steps=2,  # Simulates a larger batch size
    weight_decay=0.01,            # Regularization
    fp16=True,                    # Mixed precision for faster training
    save_total_limit=2,           # Save only 2 checkpoints to save storage
    report_to="none",             # Disable reporting to external tools
)

# Step 6: Trainer Setup
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

# Step 7: Fine-Tune the Model
print("Starting training...")
trainer.train()

# Step 8: Save Fine-Tuned Model
model.save_pretrained("./fine_tuned_gpt2")
tokenizer.save_pretrained("./fine_tuned_gpt2")
print("Model fine-tuned and saved successfully!")

# Step 9: Test the Fine-Tuned Model
from transformers import pipeline

print("Testing the fine-tuned chatbot...")
chatbot = pipeline("text-generation", model="./fine_tuned_gpt2", tokenizer=tokenizer)

while True:
    user_input = input("You: ")
    if user_input.lower() == "exit":
        print("Chatbot: Goodbye!")
        break
    response = chatbot(f"User: {user_input}\nBot:", max_length=150, num_return_sequences=1)
    print("Chatbot:", response[0]["generated_text"].replace("User:", "").replace("Bot:", "").strip())

special_token = "<|endoftext|>"
formatted_data = [
    f"User: {example['user']}\nBot: {example['bot']}{special_token}" for example in data
]

from transformers import AutoModelForCausalLM, AutoTokenizer

# Load the fine-tuned model and tokenizer
model = AutoModelForCausalLM.from_pretrained("./fine_tuned_gpt2")
tokenizer = AutoTokenizer.from_pretrained("./fine_tuned_gpt2")
special_token = "<|endoftext|>"

# Set pad_token_id if not already set
if tokenizer.pad_token_id is None:
    tokenizer.pad_token_id = tokenizer.eos_token_id

# Chat loop
while True:
    user_input = input("You: ")  # Get user input
    if user_input.lower() == "exit":  # Exit condition
        print("Chatbot: Goodbye!")
        break

    # Tokenize user input
    input_text = f"User: {user_input}\nBot:"
    input_ids = tokenizer.encode(input_text, return_tensors="pt")  # Convert to tensor

    # Create attention mask
    attention_mask = input_ids.ne(tokenizer.pad_token_id)  # Identify valid tokens

    # Generation configuration
    generation_config = {
        "max_length": len(input_ids[0]) + 100,  # Allow extra tokens for the response
        "num_return_sequences": 1,             # Generate a single response
        "temperature": 0.8,                    # Add randomness to the response
        "top_p": 0.9,                          # Use nucleus sampling
        "do_sample": True                      # Enable sampling-based generation
    }

    # Generate response
    output = model.generate(
        input_ids=input_ids,
        attention_mask=attention_mask,
        **generation_config
    )

    # Decode and format the response
    response = tokenizer.decode(output[0], skip_special_tokens=True)
    bot_response = response.replace(input_text, "").strip()  # Remove user input from output
    print("Chatbot:", bot_response)

# Commented out IPython magic to ensure Python compatibility.
# # prompt: Using streamlit create a GUI interface to have this conversation with our finetuned model
# 
# !pip install streamlit pyngrok transformers
# 
# # Assuming your fine-tuned model and tokenizer are saved in the directory "./fine_tuned_gpt2"
# # Replace with your actual paths if necessary
# 
# # Streamlit app code
# %%writefile app.py
# import streamlit as st
# from transformers import AutoModelForCausalLM, AutoTokenizer
# 
# @st.cache_resource
# def load_model_and_tokenizer():
#     model = AutoModelForCausalLM.from_pretrained("./fine_tuned_gpt2")
#     tokenizer = AutoTokenizer.from_pretrained("./fine_tuned_gpt2")
#     return model, tokenizer
# 
# model, tokenizer = load_model_and_tokenizer()
# special_token = "<|endoftext|>"
# 
# st.title("Mental Wellbeing Chatbot")
# user_input = st.text_input("You:", placeholder="Type your message here...")
# 
# if st.button("Generate Response"):
#     if user_input:
#         input_ids = tokenizer.encode(user_input + special_token, return_tensors="pt")
#         output = model.generate(input_ids, max_length=100, num_return_sequences=1, top_p=0.9, temperature=0.7)
#         response = tokenizer.decode(output[0], skip_special_tokens=True)
#         st.write(f"Chatbot: {response}")
#     else:
#         st.write("Please enter a message!")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from transformers import AutoModelForCausalLM, AutoTokenizer
# 
# # Load your fine-tuned model and tokenizer
# @st.cache_resource  # Cache the model and tokenizer for faster loading
# def load_model_and_tokenizer():
#     model = AutoModelForCausalLM.from_pretrained("./fine_tuned_gpt2")
#     tokenizer = AutoTokenizer.from_pretrained("./fine_tuned_gpt2")
#     return model, tokenizer
# 
# model, tokenizer = load_model_and_tokenizer()
# special_token = "<|endoftext|>"  # Your special token
# 
# # Streamlit app title
# st.title("Mental Wellbeing Chatbot")
# 
# # User input area
# user_input = st.text_input("You:")
# 
# # Generate response button
# if st.button("Generate Response"):
#     if user_input:
#         # Generate response using the model
#         input_ids = tokenizer.encode(user_input + special_token, return_tensors="pt")
#         output = model.generate(input_ids, max_length=100, num_return_sequences=1, top_p=0.9, temperature=0.7)  # Adjust parameters as needed
#         response = tokenizer.decode(output[0], skip_special_tokens=True)
# 
#         # Display the response
#         st.write("Chatbot:", response)
#     else:
#         st.write("Please enter a message.")

!streamlit run app.py

pip install streamlit transformers

!mkdir chatbot_app
!cd chatbot_app

from google.colab import drive
from transformers import AutoModelForCausalLM, AutoTokenizer

# Mount Google Drive
drive.mount('/content/drive')

# Define the path to save the model in Google Drive
model_save_path = '/content/drive/MyDrive/chatbot_model'

# Load your fine-tuned model and tokenizer
model = AutoModelForCausalLM.from_pretrained("gpt2")  # Use your fine-tuned model here
tokenizer = AutoTokenizer.from_pretrained("gpt2")    # Use your fine-tuned tokenizer here

# Save the model and tokenizer to Google Drive
model.save_pretrained(model_save_path)
tokenizer.save_pretrained(model_save_path)

print(f"Model and tokenizer saved to {model_save_path}")

import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer

# Mount Google Drive to access the saved model
from google.colab import drive
drive.mount('/content/drive')

# Path to your saved model in Google Drive
model_path = '/content/drive/MyDrive/chatbot_model'

# Load the fine-tuned model and tokenizer
@st.cache_resource  # Cache the model for faster reloads
def load_model_and_tokenizer():
    model = AutoModelForCausalLM.from_pretrained(model_path)
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    tokenizer.pad_token = tokenizer.eos_token  # Set pad_token if not defined
    return model, tokenizer

model, tokenizer = load_model_and_tokenizer()

# Special token
special_token = "<|endoftext|>"

# App title
st.title("Study & Wellbeing Chatbot")

# Sidebar for info
st.sidebar.title("About")
st.sidebar.info("This chatbot helps you with study tips and managing anxiety during exams.")

# User input
user_input = st.text_input("You:", placeholder="Ask me anything about studies or stress relief!")

# Generate response when button is clicked
if st.button("Get Response"):
    if user_input:
        input_text = f"User: {user_input}\nBot:"
        input_ids = tokenizer.encode(input_text, return_tensors="pt")

        # Generate response
        output = model.generate(
            input_ids,
            max_length=150,
            num_return_sequences=1,
            temperature=0.8,
            top_p=0.9,
            do_sample=True
        )

        # Decode and display the response
        response = tokenizer.decode(output[0], skip_special_tokens=True)
        bot_response = response.replace(input_text, "").strip()
        st.write(f"Chatbot: {bot_response}")
    else:
        st.write("Please enter a message.")

# Footer
st.sidebar.text("Developed with ❤️ using Streamlit and Transformers")

!streamlit run app.py

!git init
!git add .
!git commit -m "Initial commit"
!git branch -M main
!git remote add origin <https://github.com/rahusharma2312/pkl>
!git push -u origin main
